from typing import List, Dict, Any, Optional
from fastapi.responses import StreamingResponse
import re
import logging
import json

from .retriever import DynamicJusticIARetriever
from .prompt_builder import create_justicia_prompt
from .context_formatter import (
    format_documents_context_adaptive,
    calculate_optimal_retrieval_params,
    extract_document_sources,
    extract_unique_expedientes,
)
from .langchain_chains import (
    create_conversational_rag_chain,
    create_expediente_specific_chain,
    stream_chain_response
)
from .session_store import conversation_store

from app.llm.llm_service import consulta_general_streaming as llm_consulta_streaming

logger = logging.getLogger(__name__)


class RAGChainService:
    """
    Servicio RAG con dos flujos optimizados:
    - Flujo General: BÃºsqueda semÃ¡ntica multi-expediente con referencias contextuales
    - Flujo Expediente: RecuperaciÃ³n completa de un expediente especÃ­fico con chunks ordenados
    
    El mÃ©todo pÃºblico consulta_general_streaming() decide automÃ¡ticamente el flujo segÃºn expediente_filter.
    """

    def __init__(self):
        self.retriever = None

    async def consulta_general_streaming(self, pregunta: str, top_k: int = 15, conversation_context: str = "", expediente_filter: str = ""):
        """
        MÃ‰TODO PRINCIPAL - Decide automÃ¡ticamente el flujo segÃºn expediente_filter.
        
        Args:
            pregunta: Pregunta del usuario
            top_k: NÃºmero mÃ¡ximo de documentos (usado solo en flujo general)
            conversation_context: Historial de conversaciÃ³n formateado
            expediente_filter: NÃºmero de expediente especÃ­fico (opcional)
        
        Returns:
            StreamingResponse con la respuesta del LLM
        """
        logger.info(f"RAG SERVICE - Nueva consulta: '{pregunta[:50]}...'")
        logger.info(f"RAG SERVICE - ParÃ¡metros: top_k={top_k}, context_len={len(conversation_context)}, expediente={expediente_filter or 'None'}")
        
        # DECISIÃ“N DE FLUJO AUTOMÃTICA
        if expediente_filter and expediente_filter.strip():
            logger.info(f"FLUJO â†’ EXPEDIENTE ESPECÃFICO: {expediente_filter}")
            return await self._consulta_expediente_streaming(
                pregunta=pregunta,
                expediente_numero=expediente_filter.strip(),
                conversation_context=conversation_context
            )
        else:
            logger.info(f"FLUJO â†’ GENERAL: BÃºsqueda semÃ¡ntica multi-expediente")
            return await self._consulta_general_streaming(
                pregunta=pregunta,
                top_k=top_k,
                conversation_context=conversation_context
            )

    async def _consulta_general_streaming(self, pregunta: str, top_k: int = 15, conversation_context: str = ""):
        """
        FLUJO GENERAL: BÃºsqueda semÃ¡ntica con resoluciÃ³n de referencias contextuales.
        
        Optimizaciones:
        - Usa solo la pregunta actual para bÃºsqueda vectorial
        - Resuelve referencias como "Ãºltimo caso", "ese expediente"
        - Top-k adaptativo segÃºn longitud de consulta
        - Threshold 0.3 (mÃ¡s estricto)
        """
        logger.info(f"GENERAL - Pregunta: '{pregunta}'")
        logger.info(f"GENERAL - Contexto: {len(conversation_context)} chars")
        
        # SEPARACIÃ“N CRÃTICA: Usar SOLO la pregunta actual para bÃºsqueda vectorial
        search_query = pregunta.strip()

        logger.info(f"BÃšSQUEDA EN BD: '{search_query}' (sin contexto histÃ³rico)")
        logger.info(f"CONTEXTO HISTÃ“RICO: {'SÃ' if conversation_context else 'NO'} ({len(conversation_context)} chars)")

        # Crear retriever con parÃ¡metros optimizados
        self.retriever = DynamicJusticIARetriever(
            top_k=top_k,
            similarity_threshold=0.3
        )
        
        # BÃºsqueda vectorial
        docs = await self.retriever._aget_relevant_documents(search_query)
        
        logger.info(f"GENERAL - {len(docs)} documentos encontrados")
        if docs:
            for i, doc in enumerate(docs[:3]):  # Log primeros 3
                preview = doc.page_content[:100].replace('\n', ' ')
                logger.debug(f"   {i+1}. {preview}...")

        if not docs:
            logger.warning(f"GENERAL - Sin resultados para: '{pregunta}'")
            async def empty_generator():
                import json
                error_data = {
                    "type": "error", 
                    "content": "No se encontrÃ³ informaciÃ³n relevante para tu consulta.",
                    "done": True
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            return StreamingResponse(empty_generator(), media_type="text/event-stream")

        # Formateo adaptativo del contexto
        context = format_documents_context_adaptive(
            docs, 
            query=pregunta,
            context_importance="high"
        )

        # Prompt con historial
        prompt_text = create_justicia_prompt(
            pregunta=pregunta, 
            context=context, 
            conversation_context=conversation_context
        )

        # Streaming al LLM
        logger.info(f"GENERAL - Enviando prompt al LLM ({len(prompt_text)} chars)")
        return await llm_consulta_streaming(prompt_text)

    async def _consulta_expediente_streaming(self, pregunta: str, expediente_numero: str, conversation_context: str = ""):
        """
        FLUJO EXPEDIENTE: Recupera TODOS los documentos del expediente especÃ­fico.
        
        Optimizaciones:
        - Obtiene documentos completos del expediente (hasta 100)
        - Formateo estructurado por chunks con orden secuencial
        - Threshold 0.1 (mÃ¡s permisivo)
        - Top-k: 50 en fallback
        - Instrucciones especiales en el prompt
        """
        logger.info(f"EXPEDIENTE - NÃºmero: {expediente_numero}")
        logger.info(f"EXPEDIENTE - Pregunta: '{pregunta}'")
        
        # Validar formato de expediente
        expediente_pattern = r'\b\d{4}-\d{6}-\d{4}-[A-Z]{2}\b'
        if not re.match(expediente_pattern, expediente_numero):
            logger.error(f"EXPEDIENTE - Formato invÃ¡lido: {expediente_numero}")
            logger.info(f"EXPEDIENTE - Fallback a bÃºsqueda general")
            # Fallback a general
            return await self._consulta_general_streaming(
                pregunta=pregunta,
                top_k=15,
                conversation_context=conversation_context
            )
        
        try:
            from app.vectorstore.operations import get_expedient_documents, search_by_text
            
            # Obtener TODOS los documentos del expediente
            complete_docs = await get_expedient_documents(expediente_numero)
            
            if not complete_docs or len(complete_docs) == 0:
                logger.warning(f"EXPEDIENTE - No encontrado en BD: {expediente_numero}")
                logger.info(f"EXPEDIENTE - Intentando bÃºsqueda semÃ¡ntica con filtro")
    
                # Fallback: bÃºsqueda semÃ¡ntica con menciÃ³n del expediente
                search_query = f"Expediente {expediente_numero}: {pregunta}"
                complete_docs = await search_by_text(
                    query_text=search_query,
                    top_k=50,  # MÃ¡s documentos para expedientes
                    score_threshold=0.1  # MÃ¡s permisivo
                )
                
                if not complete_docs:
                    logger.error(f"EXPEDIENTE - No se encontrÃ³ en bÃºsqueda semÃ¡ntica tampoco")
                    async def not_found_generator():
                        import json
                        error_data = {
                            "type": "error",
                            "content": f"No se encontrÃ³ el expediente {expediente_numero} en la base de datos.",
                            "done": True
                        }
                        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
                    return StreamingResponse(not_found_generator(), media_type="text/event-stream")

            logger.info(f"EXPEDIENTE - {len(complete_docs)} documentos recuperados")

            # Formateo adaptativo del contexto (sin chunk_context_builder)
            context = format_documents_context_adaptive(
                complete_docs,
                query=pregunta,
                context_importance="high"
            )

            logger.info(f"EXPEDIENTE - Contexto formateado: {len(context)} chars")

            # Prompt con instrucciones especiales para expediente
            expediente_instruction = (
                f"ANÃLISIS DE EXPEDIENTE ESPECÃFICO: {expediente_numero}\n"
                f"Tienes acceso a {len(complete_docs)} documentos completos de este expediente.\n"
                f"Los documentos estÃ¡n organizados secuencialmente por tipo (demandas, resoluciones, audio transcrito).\n"
                f"Los chunks de cada documento estÃ¡n en orden cronolÃ³gico.\n"
                f"Usa esta estructura para dar respuestas precisas y contextualizadas.\n"
                f"Si la pregunta solicita informaciÃ³n especÃ­fica, bÃºscala exhaustivamente en todos los documentos."
            )
            
            # Crear prompt base
            prompt_text = create_justicia_prompt(
                pregunta=pregunta,
                context=context,
                conversation_context=conversation_context
            )
            
            # Agregar instrucciÃ³n especial al inicio
            prompt_text = expediente_instruction + "\n\n" + prompt_text
            
            # Streaming al LLM
            logger.info(f"EXPEDIENTE - Enviando prompt al LLM ({len(prompt_text)} chars)")
            return await llm_consulta_streaming(prompt_text)
            
        except Exception as e:
            logger.error(f"EXPEDIENTE - Error inesperado: {e}", exc_info=True)
            # Fallback a general
            logger.info(f"EXPEDIENTE - Fallback a bÃºsqueda general por error")
            return await self._consulta_general_streaming(
                pregunta=pregunta,
                top_k=15,
                conversation_context=conversation_context
            )
    
    async def responder_solo_con_contexto(self, pregunta: str, conversation_context: str = ""):
        """
        Responde usando SOLO el contexto de conversaciÃ³n previo, sin buscar en la BD.
        Ãštil para preguntas que se refieren exclusivamente al historial.
        """
        logger.info(f"ðŸ“š SOLO CONTEXTO - Pregunta: '{pregunta}'")
        logger.info(f"ðŸ“š SOLO CONTEXTO - Sin bÃºsqueda en BD")
        
        if not conversation_context:
            logger.warning(f"âš ï¸ SOLO CONTEXTO - No hay contexto previo disponible")
            async def no_context_generator():
                import json
                error_data = {
                    "type": "error", 
                    "content": "No hay contexto previo disponible para responder esta consulta.",
                    "done": True
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            return StreamingResponse(no_context_generator(), media_type="text/event-stream")
        
        # Prompt que usa SOLO el historial
        prompt_context_only = f"""Eres JusticIA, un asistente especializado en derecho costarricense.

El usuario te estÃ¡ haciendo una pregunta sobre informaciÃ³n que ya discutimos previamente en esta conversaciÃ³n.

HISTORIAL DE LA CONVERSACIÃ“N:
{conversation_context}

NUEVA PREGUNTA DEL USUARIO:
{pregunta}

INSTRUCCIONES:
- Responde ÃšNICAMENTE basÃ¡ndote en la informaciÃ³n del historial de conversaciÃ³n anterior
- NO busques informaciÃ³n nueva ni inventes datos
- Si la pregunta se refiere a "el primer caso", "el segundo expediente", etc., identifica claramente a cuÃ¡l te refieres
- Si no tienes suficiente informaciÃ³n en el historial, explica quÃ© informaciÃ³n especÃ­fica te falta
- MantÃ©n el tono profesional y preciso

Respuesta:"""

        logger.info(f"âœ… SOLO CONTEXTO - Prompt generado: {len(prompt_context_only)} chars")
        return await llm_consulta_streaming(prompt_context_only)
    
    # =====================================================================
    # NUEVOS MÃ‰TODOS CON LANGCHAIN CHAINS (Session-based)
    # =====================================================================
    
    async def consulta_con_historial_streaming(
        self,
        pregunta: str,
        session_id: str,
        top_k: int = 15,
        expediente_filter: Optional[str] = None
    ):
        """
        NUEVO MÃ‰TODO - Consulta RAG con gestiÃ³n automÃ¡tica de historial conversacional.
        
        Usa LangChain chains con RunnableWithMessageHistory para:
        - ReformulaciÃ³n automÃ¡tica de preguntas con contexto histÃ³rico
        - GestiÃ³n de sesiones automÃ¡tica (sin enviar historial desde frontend)
        - DetecciÃ³n de referencias contextuales mejorada
        
        Args:
            pregunta: Pregunta del usuario (sin contexto histÃ³rico)
            session_id: ID de la sesiÃ³n de conversaciÃ³n
            top_k: NÃºmero de documentos a recuperar
            expediente_filter: NÃºmero de expediente especÃ­fico (opcional)
        
        Returns:
            StreamingResponse con la respuesta del LLM
        
        Flujo:
        1. Backend recupera historial automÃ¡ticamente usando session_id
        2. LangChain reformula pregunta si hace referencia al historial
        3. Retriever busca documentos relevantes
        4. LLM genera respuesta con contexto histÃ³rico + documentos
        5. Backend guarda automÃ¡ticamente user message + assistant message
        """
        logger.info(f"ðŸ†• CONSULTA CON HISTORIAL - Pregunta: '{pregunta[:50]}...'")
        logger.info(f"ðŸ†• SESSION_ID: {session_id}")
        logger.info(f"ðŸ†• ParÃ¡metros: top_k={top_k}, expediente={expediente_filter or 'None'}")
        
        # Actualizar metadatos de la sesiÃ³n
        conversation_store.update_metadata(session_id)
        
        # Decidir flujo segÃºn expediente
        if expediente_filter and expediente_filter.strip():
            logger.info(f"ðŸ†• FLUJO â†’ EXPEDIENTE ESPECÃFICO: {expediente_filter}")
            return await self._consulta_expediente_con_historial(
                pregunta=pregunta,
                session_id=session_id,
                expediente_numero=expediente_filter.strip()
            )
        else:
            logger.info(f"ðŸ†• FLUJO â†’ GENERAL CON HISTORIAL")
            return await self._consulta_general_con_historial(
                pregunta=pregunta,
                session_id=session_id,
                top_k=top_k
            )
    
    async def _consulta_general_con_historial(
        self,
        pregunta: str,
        session_id: str,
        top_k: int = 50  # Aumentado para mÃ¡s contexto
    ):
        """
        Consulta general usando LangChain chains con historial automÃ¡tico.
        Usa SmartRetrieverRouter (V2) que decide automÃ¡ticamente el modo.
        """
        print(f"\n{'='*80}")
        print(f"ðŸ”„ CONSULTA GENERAL CON HISTORIAL")
        print(f"   - Pregunta: '{pregunta}'")
        print(f"   - Session ID: {session_id}")
        print(f"   - Top-K: {top_k}")
        print(f"{'='*80}\n")
        
        logger.info(f"ðŸ”„ GENERAL CON HISTORIAL - Creando chain conversacional")
        logger.info(f"ðŸ”„ Top-K configurado: {top_k}")
        
        # Crear retriever con parÃ¡metros optimizados
        retriever = DynamicJusticIARetriever(
            top_k=top_k,
            similarity_threshold=0.3
        )
        
        logger.info(f"âœ… DynamicJusticIARetriever creado")
        
        # Crear chain conversacional
        chain = await create_conversational_rag_chain(
            retriever=retriever,
            with_history=True
        )
        
        logger.info(f"âœ… Chain conversacional creada")
        
        # ConfiguraciÃ³n de sesiÃ³n
        config = {
            "configurable": {
                "session_id": session_id
            }
        }
        
        # Input para la chain
        input_dict = {
            "input": pregunta
        }
        
        # Streaming response
        async def event_generator():
            try:
                async for chunk in stream_chain_response(chain, input_dict, config):
                    yield chunk
                
                # Auto-generar tÃ­tulo si es el primer mensaje
                conversation_store.auto_generate_title(session_id)
                
            except Exception as e:
                logger.error(f"âŒ Error en streaming con historial: {e}", exc_info=True)
                error_data = {
                    "type": "error",
                    "content": f"Error al procesar la consulta: {str(e)}",
                    "done": True
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )
    
    async def _consulta_expediente_con_historial(
        self,
        pregunta: str,
        session_id: str,
        expediente_numero: str
    ):
        """
        Consulta de expediente especÃ­fico usando LangChain chains con historial.
        """
        logger.info(f"ðŸ“‚ EXPEDIENTE CON HISTORIAL - NÃºmero: {expediente_numero}")
        
        # Validar formato
        expediente_pattern = r'\b\d{4}-\d{6}-\d{4}-[A-Z]{2}\b'
        if not re.match(expediente_pattern, expediente_numero):
            logger.error(f"âŒ Formato invÃ¡lido: {expediente_numero}")
            # Fallback a general
            return await self._consulta_general_con_historial(
                pregunta=pregunta,
                session_id=session_id,
                top_k=15
            )
        
        # Actualizar metadatos con nÃºmero de expediente
        conversation_store.update_metadata(
            session_id=session_id,
            expediente_number=expediente_numero
        )
        
        # Crear retriever configurado para expediente especÃ­fico
        retriever = DynamicJusticIARetriever(
            top_k=50,
            similarity_threshold=0.2,
            expediente_filter=expediente_numero
        )
        
        logger.info(f"âœ… DynamicJusticIARetriever creado para expediente {expediente_numero}")
        
        # Crear chain especializada para expedientes
        chain = await create_expediente_specific_chain(
            retriever=retriever,
            expediente_numero=expediente_numero,
            with_history=True
        )
        
        logger.info(f"âœ… Chain expediente creada")
        
        # ConfiguraciÃ³n de sesiÃ³n
        config = {
            "configurable": {
                "session_id": session_id
            }
        }
        
        # Input para la chain
        input_dict = {
            "input": pregunta
        }
        
        # Streaming response
        async def event_generator():
            try:
                async for chunk in stream_chain_response(chain, input_dict, config):
                    yield chunk
                
                # Auto-generar tÃ­tulo si es el primer mensaje
                conversation_store.auto_generate_title(session_id)
                
            except Exception as e:
                logger.error(f"âŒ Error en streaming expediente con historial: {e}", exc_info=True)
                error_data = {
                    "type": "error",
                    "content": f"Error al procesar la consulta del expediente: {str(e)}",
                    "done": True
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*",
            }
        )

_rag_service = None

async def get_rag_service() -> RAGChainService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGChainService()
    return _rag_service
