from typing import List, Dict, Any
from fastapi.responses import StreamingResponse

from .retriever import JusticIARetriever
from .prompt_builder import create_justicia_prompt
from .context_formatter import (
    format_documents_context_adaptive,
    calculate_optimal_retrieval_params,
    extract_document_sources,
    extract_unique_expedientes,
)

from app.llm.llm_service import consulta_general_streaming as llm_consulta_streaming


class RAGChainService:
    "Servicio RAG que arma el prompt con contexto recuperado y delega el streaming al LLM."

    def __init__(self):
        self.retriever = None

    async def consulta_general_streaming(self, pregunta: str, top_k: int = 15, conversation_context: str = ""):
        # SEPARACIÃ“N CRÃTICA: Usar SOLO la pregunta actual para buscar en la BD
        # El contexto de conversaciÃ³n se usa Ãºnicamente para generar la respuesta
        search_query = pregunta.strip()
        
        print(f"ðŸ” BÃšSQUEDA EN BD: '{search_query}' (sin contexto histÃ³rico)")
        print(f"ðŸ“‹ CONTEXTO HISTÃ“RICO: {'SÃ' if conversation_context else 'NO'} ({len(conversation_context)} chars)")

        # Calcular parÃ¡metros de retrieval basado en la pregunta actual
        optimal_params = calculate_optimal_retrieval_params(len(search_query), context_importance="high")
        effective_top_k = min(optimal_params.get("top_k", top_k), top_k)

        retriever = JusticIARetriever(top_k=effective_top_k)
        # USAR SOLO LA PREGUNTA ACTUAL PARA LA BÃšSQUEDA VECTORIAL
        docs = await retriever._aget_relevant_documents(search_query)
        
        print(f"ðŸ“„ DOCUMENTOS ENCONTRADOS: {len(docs)} documentos para '{search_query}'")
        if docs:
            for i, doc in enumerate(docs[:3]):  # Mostrar solo los primeros 3
                preview = doc.page_content[:100].replace('\n', ' ')
                print(f"   {i+1}. {preview}...")

        if not docs:
            async def empty_generator():
                import json

                error_data = {"type": "error", "content": "No se encontrÃ³ informaciÃ³n relevante.", "done": True}
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

            return StreamingResponse(empty_generator(), media_type="text/event-stream")

        # Formatear contexto recuperado
        context = format_documents_context_adaptive(docs, query=pregunta, context_importance="high")

        # Crear prompt final que incluye pregunta, contexto recuperado y conversation_context
        prompt_text = create_justicia_prompt(pregunta=pregunta, context=context, conversation_context=conversation_context)

        # Delegar el streaming al mÃ³dulo LLM (que debe devolver un StreamingResponse)
        return await llm_consulta_streaming(prompt_text)
    
    async def responder_solo_con_contexto(self, pregunta: str, conversation_context: str = ""):
        """
        Responde usando SOLO el contexto de conversaciÃ³n previo, sin buscar en la BD.
        Usado cuando se detecta que la pregunta se refiere exclusivamente al contexto previo.
        """
        print(f"ðŸ“š RESPONDIENDO SOLO CON CONTEXTO PREVIO")
        print(f"âŒ Sin bÃºsqueda en BD para: '{pregunta}'")
        print(f"ðŸ“‹ Usando contexto histÃ³rico: {len(conversation_context)} chars")
        
        if not conversation_context:
            async def no_context_generator():
                import json
                error_data = {
                    "type": "error", 
                    "content": "No hay contexto previo disponible para responder esta consulta.",
                    "done": True
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(no_context_generator(), media_type="text/event-stream")
        
        # Crear prompt que usa SOLO el contexto de conversaciÃ³n
        prompt_context_only = f"""Eres JusticIA, un asistente especializado en derecho costarricense.

El usuario te estÃ¡ haciendo una pregunta sobre informaciÃ³n que ya discutimos previamente en esta conversaciÃ³n.

HISTORIAL DE LA CONVERSACIÃ“N:
{conversation_context}

NUEVA PREGUNTA DEL USUARIO:
{pregunta}

INSTRUCCIONES:
- Responde ÃšNICAMENTE basÃ¡ndote en la informaciÃ³n del historial de conversaciÃ³n anterior
- NO busques informaciÃ³n nueva ni inventes datos
- Si la pregunta se refiere a "el primer caso", "el segundo expediente", etc., identifica claramente a cuÃ¡l te refieres del historial
- Si no tienes suficiente informaciÃ³n en el historial, explica quÃ© informaciÃ³n especÃ­fica te falta
- MantÃ©n el tono profesional y preciso

Respuesta:"""

        print(f"ðŸŽ¯ Prompt para contexto only: {len(prompt_context_only)} chars")
        
        # Usar el LLM solo con el contexto de conversaciÃ³n
        return await llm_consulta_streaming(prompt_context_only)

_rag_service = None

async def get_rag_service() -> RAGChainService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGChainService()
    return _rag_service
