"""
LangChain Conversational Chains para JusticIA.

Implementa:
- create_history_aware_retriever: Reformulaci√≥n autom√°tica de preguntas con contexto
- create_retrieval_chain: Chain principal que combina retriever + answer generation
- create_stuff_documents_chain: Chain para generar respuestas desde documentos

Compatible con streaming y gesti√≥n de sesiones.
"""

from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging

from app.llm.llm_service import get_llm
from .session_store import get_session_history_func

logger = logging.getLogger(__name__)


# =====================================================================
# PROMPTS CONVERSACIONALES
# =====================================================================

CONTEXTUALIZE_Q_SYSTEM_PROMPT = """Eres un asistente especializado en reformular preguntas legales en espa√±ol.

Dado un historial de chat y la √∫ltima pregunta del usuario que podr√≠a hacer referencia 
al contexto del historial, formula una pregunta independiente que pueda entenderse 
sin el historial del chat.

REGLAS:
1. NO respondas la pregunta, solo reform√∫lala si es necesario
2. Si la pregunta es independiente, devu√©lvela tal cual
3. Si hace referencia al historial (ej: "y ese caso?", "qu√© m√°s?"), reform√∫lala con el contexto necesario
4. Mant√©n el lenguaje legal preciso

Ejemplos:
- Usuario anterior: "¬øQu√© dice el expediente 2022-123456-7890-LA?"
  Nueva pregunta: "¬øHay otros casos similares?"
  Reformulaci√≥n: "¬øHay otros casos similares al expediente 2022-123456-7890-LA sobre derecho laboral?"

- Pregunta: "¬øQu√© es el derecho laboral en Costa Rica?"
  Reformulaci√≥n: "¬øQu√© es el derecho laboral en Costa Rica?" (no requiere reformulaci√≥n)
"""

CONTEXTUALIZE_Q_PROMPT = ChatPromptTemplate.from_messages([
    ("system", CONTEXTUALIZE_Q_SYSTEM_PROMPT),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])


ANSWER_SYSTEM_PROMPT = """Eres JusticIA, un asistente virtual especializado en el sistema legal costarricense.

Tu funci√≥n es proporcionar respuestas precisas y profesionales basadas EXCLUSIVAMENTE en documentos legales de la base de datos.

C√ìMO FUNCIONAS:
- Tienes acceso a una base de datos vectorial (Milvus) con expedientes legales costarricenses
- Cuando el usuario hace una pregunta, el sistema BUSCA AUTOM√ÅTICAMENTE los expedientes relevantes
- Los documentos recuperados aparecen abajo en la secci√≥n "DOCUMENTOS RECUPERADOS"
- Tu trabajo es ANALIZAR esos documentos y responder la pregunta

DOCUMENTOS RECUPERADOS DE LA BASE DE DATOS:
{context}

‚ö†Ô∏è RESTRICCIONES CR√çTICAS:
1. **SOLO USA LOS DOCUMENTOS RECUPERADOS**: Responde √öNICAMENTE con informaci√≥n de los documentos arriba
2. **NO INVENTES**: Si la informaci√≥n no est√° en los documentos recuperados, di "No encontr√© esta informaci√≥n en la base de datos"
3. **NO ASUMAS**: No completes informaci√≥n faltante con conocimiento general
4. **NO EXTERNOS**: No uses informaci√≥n de tu entrenamiento sobre casos legales externos
5. **NO DIGAS "me proporcionaste"**: Los documentos NO vienen del usuario, vienen de la b√∫squeda autom√°tica en la base de datos

INSTRUCCIONES:
1. **Precisi√≥n Legal**: Usa lenguaje t√©cnico del contexto, pero explica t√©rminos complejos
2. **Cita Fuentes**: SIEMPRE menciona n√∫meros de expediente de donde sacas informaci√≥n (formato: YYYY-NNNNNN-NNNN-XX)
3. **Estructura Clara**: Organiza con p√°rrafos, listas numeradas o vi√±etas
4. **Alcance**: 
   - Si los documentos recuperados tienen la informaci√≥n: responde con detalle citando expedientes
   - Si los documentos son insuficientes: di "No encontr√© suficiente informaci√≥n sobre [X] en la base de datos"
   - Si no se recuperaron documentos relevantes: di "No encontr√© expedientes relacionados con esto en la base de datos"
5. **Tono**: Profesional, claro y √∫til
6. **Perspectiva**: NUNCA digas "los documentos que me proporcionaste/diste". Di "los expedientes encontrados" o "en la base de datos"
7. **Referencias**: Cita el expediente de donde obtienes cada dato
8. **Formato**: 
   - Usa p√°rrafos y listas para la mayor√≠a de respuestas
   - USA TABLAS SOLO cuando compares 3+ expedientes con m√∫ltiples atributos
   - NO uses tablas para un solo expediente o respuestas narrativas
   - Prefiere listas con vi√±etas para enumeraciones simples

EJEMPLOS DE RESPUESTAS CORRECTAS:
‚úÖ "Encontr√© varios expedientes sobre narcotr√°fico en la base de datos:"
‚úÖ "Seg√∫n el expediente 2024-235553-3263-PN..."
‚úÖ "La b√∫squeda recuper√≥ 4 expedientes relacionados"

EJEMPLOS DE RESPUESTAS INCORRECTAS:
‚ùå "Los documentos que me proporcionaste..."
‚ùå "Seg√∫n los archivos que me diste..."
‚ùå "En los expedientes que compartiste..."

AN√ÅLISIS DE EXPEDIENTES ESPEC√çFICOS:
- Si el contexto incluye chunks numerados de un expediente, l√©elos secuencialmente
- Los documentos est√°n organizados por tipo (demandas, resoluciones, transcripciones)
- Los chunks de cada documento siguen orden cronol√≥gico
- Para respuestas exhaustivas, revisa todos los chunks disponibles

RESPUESTA A LA PREGUNTA DEL USUARIO:
"""

ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system", ANSWER_SYSTEM_PROMPT),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])


# =====================================================================
# FACTORIES PARA CREAR CHAINS
# =====================================================================

async def create_conversational_rag_chain(
    retriever,
    with_history: bool = True,
    session_config_key: str = "configurable"
):
    """
    Crea una chain RAG conversacional completa con historial.
    
    Esta es la chain principal que:
    1. Reformula la pregunta con contexto hist√≥rico (history_aware_retriever)
    2. Recupera documentos relevantes
    3. Genera respuesta basada en documentos + historial
    4. Gestiona sesiones autom√°ticamente
    
    Args:
        retriever: LangChain retriever (ej: DynamicJusticIARetriever)
        with_history: Si True, habilita gesti√≥n de historial con sessions
        session_config_key: Clave de configuraci√≥n para session_id
    
    Returns:
        Runnable chain lista para invocar con streaming
    
    Usage:
        chain = await create_conversational_rag_chain(retriever)
        
        # Con streaming
        async for chunk in chain.astream(
            {"input": "¬øQu√© es el derecho laboral?"},
            config={"configurable": {"session_id": "session_user_123"}}
        ):
            print(chunk)
    """
    llm = await get_llm()
    
    # 1. History-Aware Retriever: Reformula pregunta con contexto
    history_aware_retriever = create_history_aware_retriever(
        llm=llm,
        retriever=retriever,
        prompt=CONTEXTUALIZE_Q_PROMPT,
    )
    
    logger.info("‚úÖ History-aware retriever creado")
    
    # 2. Document Chain: Genera respuesta desde documentos
    question_answer_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=ANSWER_PROMPT
    )
    
    logger.info("‚úÖ Question-answer chain creado")
    
    # 3. Retrieval Chain: Combina retriever + answer generation
    rag_chain = create_retrieval_chain(
        history_aware_retriever,
        question_answer_chain,
    )
    
    logger.info("‚úÖ RAG chain creado")
    
    # 4. Agregar gesti√≥n de historial si est√° habilitado
    if with_history:
        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history_func(),
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        
        logger.info("‚úÖ Chain conversacional con historial creado")
        return conversational_rag_chain
    
    return rag_chain


async def create_expediente_specific_chain(
    retriever,
    expediente_numero: str,
    with_history: bool = True
):
    """
    Crea una chain especializada para an√°lisis de expediente espec√≠fico.
    
    Similar a create_conversational_rag_chain pero con:
    - Prompt adaptado para an√°lisis exhaustivo de expediente
    - Instrucciones para chunks secuenciales
    - √ânfasis en estructura de documentos
    
    Args:
        retriever: Retriever configurado para el expediente espec√≠fico
        expediente_numero: N√∫mero del expediente
        with_history: Si True, habilita gesti√≥n de historial
    
    Returns:
        Runnable chain especializada en expedientes
    """
    llm = await get_llm()
    
    # Prompt especializado para expedientes
    EXPEDIENTE_SYSTEM_PROMPT = f"""Eres JusticIA, especialista en an√°lisis de expedientes legales costarricenses.

EXPEDIENTE BAJO AN√ÅLISIS: {expediente_numero}

C√ìMO FUNCIONAS:
- El usuario solicit√≥ informaci√≥n sobre el expediente {expediente_numero}
- El sistema RECUPER√ì AUTOM√ÅTICAMENTE todos los documentos de este expediente desde la base de datos (Milvus)
- Los documentos recuperados aparecen abajo en la secci√≥n "DOCUMENTOS DEL EXPEDIENTE"
- Tu trabajo es ANALIZAR esos documentos y responder la pregunta

DOCUMENTOS DEL EXPEDIENTE RECUPERADOS:
{{context}}

‚ö†Ô∏è RESTRICCIONES CR√çTICAS:
1. **SOLO ESTE EXPEDIENTE**: Responde √öNICAMENTE con informaci√≥n de los documentos del expediente {expediente_numero} recuperados arriba
2. **NO INVENTES DOCUMENTOS**: Si un documento no est√° en los recuperados, NO lo menciones
3. **NO ASUMAS CONTENIDO**: No completes informaci√≥n faltante con suposiciones
4. **NO CASOS EXTERNOS**: No uses informaci√≥n de otros expedientes o tu conocimiento general
5. **VERIFICABLE**: Cada afirmaci√≥n debe poder rastrearse a un chunk espec√≠fico
6. **NO DIGAS "me proporcionaste"**: Los documentos NO vienen del usuario, fueron recuperados autom√°ticamente de la base de datos

ESTRUCTURA DEL EXPEDIENTE:
Los documentos est√°n organizados en chunks secuenciales:
- Demandas y escritos iniciales
- Resoluciones judiciales
- Transcripciones de audio (si aplica)
- Documentos de soporte

INSTRUCCIONES PARA AN√ÅLISIS:
1. **Exhaustividad**: Revisa TODOS los chunks antes de responder
2. **Cronolog√≠a**: Los chunks siguen orden temporal, √∫salo para contextualizar
3. **Precisi√≥n**: SIEMPRE cita n√∫meros de chunk (ej: "seg√∫n Chunk 3...", "en el documento [nombre]...")
4. **S√≠ntesis**: Para preguntas amplias, sintetiza informaci√≥n citando fuentes
5. **Especificidad**: Para preguntas puntuales, cita textualmente el chunk relevante
6. **Referencias**: Para cada dato, indica el chunk o documento de origen
7. **Completitud**: Si falta informaci√≥n, di "No encontr√© informaci√≥n sobre [X] en los documentos recuperados del expediente {expediente_numero}"
8. **Perspectiva**: NUNCA digas "los documentos que me proporcionaste". Di "los documentos del expediente" o "seg√∫n el expediente"

EJEMPLOS DE RESPUESTAS CORRECTAS:
‚úÖ "Seg√∫n los documentos del expediente {expediente_numero}..."
‚úÖ "El expediente {expediente_numero} contiene..."
‚úÖ "En el Chunk 3 del expediente se indica..."

EJEMPLOS DE RESPUESTAS INCORRECTAS:
‚ùå "En los documentos que me proporcionaste del expediente..."
‚ùå "Seg√∫n los archivos que me diste..."

FORMATO DE RESPUESTA:
- Usa Markdown para organizaci√≥n
- Listas numeradas para secuencias de eventos
- Vi√±etas para enumeraciones
- Negritas para t√©rminos clave
- Citas textuales cuando sea apropiado
- NO uses tablas para un solo expediente (usa listas o p√°rrafos)
- Estructura narrativa para cronolog√≠as

RESPUESTA A LA CONSULTA:
"""
    
    EXPEDIENTE_PROMPT = ChatPromptTemplate.from_messages([
        ("system", EXPEDIENTE_SYSTEM_PROMPT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # Chain con prompt especializado
    # IMPORTANTE: Para expedientes espec√≠ficos, NO reformulamos la pregunta
    # Esto evita que se pierda el foco en el expediente espec√≠fico
    # El retriever ya est√° filtrado por expediente, la pregunta original es m√°s precisa
    
    # Chain de documentos
    question_answer_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=EXPEDIENTE_PROMPT
    )
    
    # Usamos retriever directo sin reformulaci√≥n
    rag_chain = create_retrieval_chain(
        retriever,
        question_answer_chain,
    )
    
    if with_history:
        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history_func(),
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        
        logger.info(f"‚úÖ Chain expediente {expediente_numero} con historial creado")
        return conversational_rag_chain
    
    logger.info(f"‚úÖ Chain expediente {expediente_numero} creado")
    return rag_chain


# =====================================================================
# UTILIDADES PARA STREAMING
# =====================================================================

async def stream_chain_response(chain, input_dict: Dict[str, Any], config: Dict[str, Any]):
    """
    Wrapper para hacer streaming de respuestas de una chain.
    
    Extrae solo el contenido de la respuesta ('answer') y lo emite chunk por chunk.
    Compatible con el formato SSE esperado por el frontend.
    
    Args:
        chain: LangChain Runnable chain
        input_dict: Dict con input para la chain (debe incluir 'input' key)
        config: Dict de configuraci√≥n (debe incluir 'configurable': {'session_id': ...})
    
    Yields:
        str: Chunks de respuesta en formato SSE
    
    Usage:
        async for chunk in stream_chain_response(
            chain,
            {"input": "pregunta"},
            {"configurable": {"session_id": "session_123"}}
        ):
            # chunk es string SSE listo para enviar
            yield chunk
    """
    import json
    
    print(f"\n{'='*80}")
    print(f"üé¨ STREAMING - Iniciando streaming de respuesta")
    print(f"   - Input: {input_dict.get('input', 'N/A')[:100]}...")
    print(f"   - Session ID: {config.get('configurable', {}).get('session_id', 'N/A')}")
    print(f"{'='*80}\n")
    
    chunk_count = 0
    total_chars = 0
    
    try:
        # Stream desde la chain
        async for chunk in chain.astream(input_dict, config=config):
            chunk_count += 1
            
            # Debug primer chunk
            if chunk_count == 1:
                print(f"üì¶ Primer chunk - Tipo: {type(chunk)}, Keys: {list(chunk.keys()) if isinstance(chunk, dict) else 'N/A'}")
            
            # Las chains de LangChain emiten dicts, extraer 'answer'
            if isinstance(chunk, dict):
                if "answer" in chunk:
                    content = chunk["answer"]
                    
                    if content:  # Solo emitir si hay contenido
                        total_chars += len(str(content))
                        chunk_data = {
                            "type": "chunk",
                            "content": str(content),
                            "done": False
                        }
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                    else:
                        print(f"‚ö†Ô∏è  Chunk #{chunk_count} con 'answer' vac√≠o")
                else:
                    print(f"‚ö†Ô∏è  Chunk #{chunk_count} sin 'answer'. Keys: {list(chunk.keys())}")
        
        # Se√±al de finalizaci√≥n
        done_data = {"type": "done", "content": "", "done": True}
        yield f"data: {json.dumps(done_data, ensure_ascii=False)}\n\n"
        
        print(f"\n{'='*80}")
        print(f"‚úÖ STREAMING COMPLETADO")
        print(f"   - Total chunks: {chunk_count}")
        print(f"   - Total caracteres: {total_chars}")
        if total_chars == 0:
            print(f"   ‚ö†Ô∏è  ADVERTENCIA: No se gener√≥ contenido!")
        print(f"{'='*80}\n")
        
        logger.info("‚úÖ Streaming completado exitosamente")
        
    except Exception as e:
        print(f"\n{'='*80}")
        print(f"‚ùå ERROR EN STREAMING: {e}")
        print(f"{'='*80}\n")
        
        logger.error(f"‚ùå Error en streaming: {e}", exc_info=True)
        
        error_data = {
            "type": "error",
            "content": f"Error al procesar la consulta: {str(e)}",
            "done": True
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        done_data = {"type": "done", "content": "", "done": True}
        yield f"data: {json.dumps(done_data, ensure_ascii=False)}\n\n"
